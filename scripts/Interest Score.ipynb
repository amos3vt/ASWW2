{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the Interest of a Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Score responses by how interesting they may be.\n",
    "\n",
    "**Properties of an Interesting Response (for our purposes)**:\n",
    "- greater length\n",
    "- variation of ideas (not paraphrasing the same sentences repeatedly)\n",
    "- discussion of certain topics\n",
    "  - race\n",
    "  - gender\n",
    "  - combat\n",
    "  - training\n",
    "- lack of discussion of certain topics\n",
    "  - general comments about the survey itself\n",
    "- clarity of response\n",
    "  - few \"unclear\" tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of example, the responses to survey 64 will be used to develop an interest scoring method which, ideally, would be easily extendable to other surveys in the ASWWII corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Survey 64 data\n",
    "path = \"../data/surveys_interest.xlsx\"\n",
    "df = pd.read_excel(path, na_filter=False, usecols=['T4'], dtype={'T4': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Subscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider the length of a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['T4'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use BERT to score the similarity of sentences in a response where a higher score would indicate a lot of redundancies in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Savannah\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Savannah\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Savannah\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Savannah\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Savannah\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Savannah\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def similarity(res):\n",
    "    \n",
    "    # Split response into sentences\n",
    "    # TODO: Find a better way to handle abbreviations\n",
    "    res_split = nltk.sent_tokenize(res)\n",
    "    \n",
    "    # Too few sentences to do any comparison\n",
    "    if len(res_split) < 2:\n",
    "        return 0,0,0,0\n",
    "    \n",
    "    # Embed each sentence\n",
    "    res_split_embed = model.encode(res_split)\n",
    "    \n",
    "    # Compare combinations of sentences\n",
    "    sim = [1 - cosine(c[0], c[1]) for c in combinations(res_split_embed,2)]\n",
    "    \n",
    "    # Not sure whether the min, max, mean, or median\n",
    "    #   would be the most useful, so return them all\n",
    "    return np.amax(sim), np.amin(sim), np.mean(sim), np.median(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df['T4'].apply(similarity)\n",
    "df['max'] = [r[0] for r in res]\n",
    "df['min'] = [r[1] for r in res]\n",
    "df['mean'] = [r[2] for r in res]\n",
    "df['median'] = [r[3] for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use BERT again to determine if a given response discusses certain topics. For a more detailed explanation of our BERT-contextualized keyword searching method, read through the \"LongResponse_Filtering\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model.eval()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embeddings(text):\n",
    "    \n",
    "    # Tokenize the text\n",
    "    split_text = text.split(\". \")\n",
    "    marked_text = \"[CLS] \" + \" [SEP] \".join(split_text) + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)[:512] # Truncate if longer than 512\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Mark tokens belonging to a sentence\n",
    "    segment_ids = [0]*len(tokenized_text)\n",
    "    is_zero = True\n",
    "    for i in range(len(tokenized_text)):\n",
    "        segment_ids[i] = 0 if is_zero else 1\n",
    "        if tokenized_text[i] == \"[SEP]\":\n",
    "            is_zero = not is_zero\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segment_ids])\n",
    "\n",
    "    # Run through BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Adjust\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Get token vectors\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    \n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the number of words in a given text that have\n",
    "#   a high similarity to at least one keyword.\n",
    "# Also track the highest similarity scores\n",
    "\n",
    "# df: Dataframe\n",
    "# column: column to consider for labeling\n",
    "# label: name of column to store results in\n",
    "# keys: keyword dictionary\n",
    "# thresh: similarity threshold\n",
    "def label_topic(df, column, label, keys, thresh):\n",
    "\n",
    "    # Initialize/Reset column\n",
    "    df[label] = 0\n",
    "    df[label+\"_score\"] = float(\"-inf\")\n",
    "    \n",
    "    # Track tokens that matched to keywords\n",
    "    token_matches = []\n",
    "    \n",
    "    # Search\n",
    "    for i in range(len(df)):\n",
    "        best_sim = float(\"-inf\")\n",
    "        embed = get_token_embeddings(df[column][i])\n",
    "        for j in range(len(embed)):\n",
    "            for k in keys:\n",
    "                sim = 1 - cosine(embed[j], k['embed'])\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                if sim >= thresh:\n",
    "                    df.at[i, label] += 1\n",
    "                    \n",
    "                    # Get the token that matched to a keyword\n",
    "                    split_text = df[column][i].split(\". \")\n",
    "                    marked_text = \"[CLS] \" + \" [SEP] \".join(split_text) + \" [SEP]\"\n",
    "                    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "                    token_matches.append((tokenized_text[j], k['text']))\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "        df.at[i, label+\"_score\"] = best_sim\n",
    "        \n",
    "        # Track progress\n",
    "        #if i%100==0:\n",
    "        #    print(i,\"/\",len(df))\n",
    "                    \n",
    "    return token_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some topics have been predetermined to be particularly interesting or uninteresting for the prosepective audience. First, we will consider the interesting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 1: Race\n",
    "keys_race = [\n",
    "    {\"text\": \"negro soldier\", \"idx\": 1, \"embed\": None},\n",
    "    {\"text\": \"the white man\", \"idx\": 2, \"embed\": None}\n",
    "]\n",
    "\n",
    "for k in keys_race:\n",
    "    embed = get_token_embeddings(k['text'])\n",
    "    k['embed'] = embed[k['idx']]\n",
    "    \n",
    "    \n",
    "token_matches = label_topic(df, 'T4', 'topic_race', keys_race, 0.5)\n",
    "print(set(token_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 2: Gender\n",
    "keys_gender = [\n",
    "    {\"text\": \"woman\", \"idx\": 1, \"embed\": None}\n",
    "]\n",
    "\n",
    "man_embed = get_token_embeddings(\"man\")[1]\n",
    "for k in keys_gender:\n",
    "    embed = get_token_embeddings(k['text'])\n",
    "    k['embed'] = embed[k['idx']] - (np.dot(embed[k['idx']], man_embed) / np.dot(man_embed, man_embed)) * man_embed\n",
    "    \n",
    "token_matches = label_topic(df, 'T4', 'topic_gender', keys_gender, 0.3)\n",
    "print(set(token_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Expand the list of interesting topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will consider the uninteresting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 1: Survey\n",
    "keys_survey = [\n",
    "    {\"text\": \"survey\", \"idx\": 1, \"embed\": None},\n",
    "]\n",
    "\n",
    "for k in keys_survey:\n",
    "    embed = get_token_embeddings(k['text'])\n",
    "    k['embed'] = embed[k['idx']]\n",
    "    \n",
    "\n",
    "token_matches = label_topic(df, 'T4', 'topic_race', keys_survey, 0.5)\n",
    "print(set(token_matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Count number of \"unclear\", normalize by response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to excel\n",
    "df.to_excel(\"../data/surveys_interest.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create function that takes subscores and returns an interest score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT to Embed Multi-Sentence Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed individual sentences, we used Sentence-BERT which is a tool extending BERT's base functionality to give \"semantially meaningful sentence embeddings\". The resulting embeddings from this method are comparable using cosine similarity [https://arxiv.org/abs/1908.10084]. \n",
    "\n",
    "In the following section, we demonstrate the use of Sentence-BERT to obtain comparable sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-BERT Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would expect similar sentences to have similar embeddings. For each pair of example sentences, we will measure the cosine similarity of their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "example_set_1 = [\n",
    "    ['I like dogs.', 'I love dogs.'],\n",
    "    ['I like dogs.', 'I want a puppy.'],\n",
    "    ['I like dogs.', 'I like cats.'],\n",
    "    ['I like dogs.', 'I want food.'],\n",
    "    ['I like dogs.', 'The hound ran around the yard.'],\n",
    "    ['I like dogs.', \"It snowed today.\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from textwrap import fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text(example_set):\n",
    "    \n",
    "    # Initialize table\n",
    "    t = PrettyTable(['Excerpt A', 'Excerpt B', 'Similarity'])\n",
    "    \n",
    "    for e in example_set:\n",
    "        \n",
    "        # Get embeddings for example\n",
    "        embed = model.encode(e)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        sim = 1 - cosine(embed[0], embed[1])\n",
    "        \n",
    "        # Add results to table\n",
    "        t.add_row([fill(e[0], width=35), fill(e[1], width=35), round(sim,3)])\n",
    "        \n",
    "    # Display table\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------+------------+\n",
      "|  Excerpt A   |           Excerpt B            | Similarity |\n",
      "+--------------+--------------------------------+------------+\n",
      "| I like dogs. |          I love dogs.          |   0.938    |\n",
      "| I like dogs. |        I want a puppy.         |   0.651    |\n",
      "| I like dogs. |          I like cats.          |   0.417    |\n",
      "| I like dogs. |          I want food.          |   0.358    |\n",
      "| I like dogs. | The hound ran around the yard. |    0.35    |\n",
      "| I like dogs. |        It snowed today.        |   0.114    |\n",
      "+--------------+--------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "compare_text(example_set_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Multi-Sentence Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"LongResponse_Embedding\" notebooks, different methods of embedding responses that may be multi-sentence were considered. For this purpose, we will revist Method 1 which simply passes the full response to the Sentence-BERT encoding function with no special handling.\n",
    "\n",
    "This method is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set_2 = [\n",
    "    [\n",
    "        \"The ground was covered in snow. Icicles hung from branches. Snowflakes kept falling.\",\n",
    "        \"Winter has brought the coldest weather. This blizzard seems endless.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Winter has brought the coldest weather. This blizzard seems endless.\",\n",
    "        \"Rain waters the blooming flowers. Spring is finally here.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Winter has brought the coldest weather. This blizzard seems endless.\",\n",
    "        \"The table was filled with delicious food. I couldn't decide which dish to start with.\"\n",
    "    ],\n",
    "    [\n",
    "        \"The table was filled with delicious food. I couldn't decide which dish to start with.\",\n",
    "        \"The smell coming from the stove is mouth-watering.\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------+------------+\n",
      "|              Excerpt A              |              Excerpt B              | Similarity |\n",
      "+-------------------------------------+-------------------------------------+------------+\n",
      "|   The ground was covered in snow.   |    Winter has brought the coldest   |   0.604    |\n",
      "|     Icicles hung from branches.     |     weather. This blizzard seems    |            |\n",
      "|       Snowflakes kept falling.      |               endless.              |            |\n",
      "|    Winter has brought the coldest   |  Rain waters the blooming flowers.  |   0.077    |\n",
      "|     weather. This blizzard seems    |       Spring is finally here.       |            |\n",
      "|               endless.              |                                     |            |\n",
      "|    Winter has brought the coldest   | The table was filled with delicious |   0.041    |\n",
      "|     weather. This blizzard seems    |  food. I couldn't decide which dish |            |\n",
      "|               endless.              |            to start with.           |            |\n",
      "| The table was filled with delicious |  The smell coming from the stove is |   0.252    |\n",
      "|  food. I couldn't decide which dish |           mouth-watering.           |            |\n",
      "|            to start with.           |                                     |            |\n",
      "+-------------------------------------+-------------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "compare_text(example_set_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we see that the embeddings of short multi-sentence pieces are still useful. For our purposes, the longest of survey responses are still just a few sentences, so we will assume most embeddings will not be too diluted for use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
