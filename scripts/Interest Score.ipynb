{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the Interest of a Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Score responses by how interesting they may be.\n",
    "\n",
    "**Properties of an Interesting Response (for our purposes)**:\n",
    "- greater length\n",
    "- variation of ideas (not paraphrasing the same sentences repeatedly)\n",
    "- discussion of certain topics\n",
    "  - race\n",
    "  - gender\n",
    "  - combat\n",
    "  - training\n",
    "- lack of discussion of certain topics\n",
    "  - general comments about the survey itself\n",
    "- clarity of response\n",
    "  - few \"unclear\" tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of example, the responses to survey 64 will be used to develop an interest scoring method which, ideally, would be easily extendable to other surveys in the ASWWII corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Survey 64 data\n",
    "path = \"../data/surveys_interest.xlsx\"\n",
    "df = pd.read_excel(path, na_filter=False, usecols=['T4'], dtype={'T4': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Subscores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we consider the length of a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['T4'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use BERT to score the similarity of sentences in a response where a higher score would indicate a lot of redundancies in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def similarity(res):\n",
    "    \n",
    "    # Split response into sentences\n",
    "    # TODO: Find a better way to handle abbreviations\n",
    "    res_split = nltk.sent_tokenize(res)\n",
    "    \n",
    "    # Too few sentences to do any comparison\n",
    "    if len(res_split) < 2:\n",
    "        return 0,0,0,0,0\n",
    "    \n",
    "    # Embed each sentence\n",
    "    res_split_embed = model.encode(res_split)\n",
    "    \n",
    "    # Compare combinations of sentences\n",
    "    sim = [1 - cosine(c[0], c[1]) for c in combinations(res_split_embed,2)]\n",
    "    \n",
    "    # Not sure whether the min, max, mean, or median\n",
    "    #   would be the most useful, so return them all\n",
    "    # Also return rank of response embedding matrix\n",
    "    #   where each column is a different sentence\n",
    "    return np.amax(sim), np.amin(sim), np.mean(sim), np.median(sim), np.linalg.matrix_rank(np.transpose(res_split_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function and unpack results\n",
    "res = df['T4'].apply(similarity)\n",
    "df['max'] = [r[0] for r in res]\n",
    "df['min'] = [r[1] for r in res]\n",
    "df['mean'] = [r[2] for r in res]\n",
    "df['median'] = [r[3] for r in res]\n",
    "df['rank'] = [r[4] for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use BERT again to determine if a given response discusses certain topics. For a more detailed explanation of our BERT-contextualized keyword searching method, read through the \"LongResponse_Filtering\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model.eval()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embeddings(text):\n",
    "    \n",
    "    # Tokenize the text\n",
    "    split_text = text.split(\". \")\n",
    "    marked_text = \"[CLS] \" + \" [SEP] \".join(split_text) + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)[:512] # Truncate if longer than 512\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Mark tokens belonging to a sentence\n",
    "    segment_ids = [0]*len(tokenized_text)\n",
    "    is_zero = True\n",
    "    for i in range(len(tokenized_text)):\n",
    "        segment_ids[i] = 0 if is_zero else 1\n",
    "        if tokenized_text[i] == \"[SEP]\":\n",
    "            is_zero = not is_zero\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segment_ids])\n",
    "\n",
    "    # Run through BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Adjust\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Get token vectors\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    \n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the number of words in a given text that have\n",
    "#   a high similarity to at least one keyword.\n",
    "# Also track the highest similarity scores\n",
    "\n",
    "# df: Dataframe\n",
    "# column: column to consider for labeling\n",
    "# label: name of column to store results in\n",
    "# keys: keyword dictionary\n",
    "# thresh: similarity threshold\n",
    "def label_topic(df, column, label, keys, thresh):\n",
    "\n",
    "    # Initialize/Reset column\n",
    "    df[label] = 0\n",
    "    df[label+\"_score\"] = float(\"-inf\")\n",
    "    \n",
    "    # Track tokens that matched to keywords\n",
    "    token_matches = []\n",
    "    \n",
    "    # Search\n",
    "    for i in range(len(df)):\n",
    "        best_sim = float(\"-inf\")\n",
    "        embed = get_token_embeddings(df[column][i])\n",
    "        for j in range(len(embed)):\n",
    "            for k in keys:\n",
    "                sim = 1 - cosine(embed[j], k['embed'])\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                if sim >= thresh:\n",
    "                    df.at[i, label] += 1\n",
    "                    \n",
    "                    # Get the token that matched to a keyword\n",
    "                    split_text = df[column][i].split(\". \")\n",
    "                    marked_text = \"[CLS] \" + \" [SEP] \".join(split_text) + \" [SEP]\"\n",
    "                    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "                    token_matches.append((tokenized_text[j], k['text']))\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "        df.at[i, label+\"_score\"] = best_sim\n",
    "        \n",
    "        # Track progress\n",
    "        #if i%100==0:\n",
    "        #    print(i,\"/\",len(df))\n",
    "                    \n",
    "    return token_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some topics have been predetermined to be particularly interesting or uninteresting for the prosepective audience. First, we will consider the interesting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 1: Race\n",
    "keys_race = [\n",
    "    {\"text\": \"negro soldier\", \"idx\": 1, \"embed\": None},\n",
    "    {\"text\": \"the white man\", \"idx\": 2, \"embed\": None}\n",
    "]\n",
    "\n",
    "for k in keys_race:\n",
    "    embed = get_token_embeddings(k['text'])\n",
    "    k['embed'] = embed[k['idx']]\n",
    "    \n",
    "    \n",
    "token_matches = label_topic(df, 'T4', 'topic_race', keys_race, 0.5)\n",
    "print(set(token_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 2: Gender\n",
    "keys_gender = [\n",
    "    {\"text\": \"woman\", \"idx\": 1, \"embed\": None}\n",
    "]\n",
    "\n",
    "man_embed = get_token_embeddings(\"man\")[1]\n",
    "for k in keys_gender:\n",
    "    embed = get_token_embeddings(k['text'])\n",
    "    k['embed'] = embed[k['idx']] - (np.dot(embed[k['idx']], man_embed) / np.dot(man_embed, man_embed)) * man_embed\n",
    "    \n",
    "token_matches = label_topic(df, 'T4', 'topic_gender', keys_gender, 0.3)\n",
    "print(set(token_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Expand the list of interesting topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will consider the uninteresting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 1: Survey\n",
    "keys_survey = [\n",
    "    {\"text\": \"this is a good questionnaire\", \"idx\": 5, \"embed\": None},\n",
    "    {\"text\": \"too many questions\", \"idx\": 3, \"embed\": None},\n",
    "    {\"text\": \"questions about\", \"idx\": 1, \"embed\": None},\n",
    "]\n",
    "\n",
    "for k in keys_survey:\n",
    "    embed = get_token_embeddings(k['text'])\n",
    "    k['embed'] = embed[k['idx']]\n",
    "    \n",
    "\n",
    "token_matches = label_topic(df, 'T4', 'topic_survey', keys_survey, 0.52)\n",
    "print(set(token_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Expand list of uninteresting topics\n",
    "# e.g. \"no comments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clarity(res):\n",
    "    if res.count(\"unclear\") == 0:\n",
    "        return 1\n",
    "    return 1 - res.count(\"unclear\") / len(res.split())\n",
    "\n",
    "df['clarity'] = df['T4'].apply(clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to excel\n",
    "df.to_excel(\"../data/surveys_interest.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data\n",
    "import pandas as pd\n",
    "path = \"../data/surveys_interest.xlsx\"\n",
    "df = pd.read_excel(path, na_filter=False, dtype={'T4': str})\n",
    "A = df.head(200).iloc[:,1:14].values # columns from length through clarity\n",
    "b = df.head(200).iloc[:,14].values # predetermined interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fee12e8b70>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot singular values of A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "U,S,Vt = np.linalg.svd(A, full_matrices=False)\n",
    "plt.semilogy(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: Inverse Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve: Ax = b\n",
    "x = (Vt.T)*(1/S)@(U.T@b)\n",
    "\n",
    "# May adjust the values in x as needed\n",
    "\n",
    "def score(row):\n",
    "    return np.dot(row[1:14], x)\n",
    "    \n",
    "df['interest_score_v1'] = df.apply(score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to excel\n",
    "df.to_excel(\"../data/surveys_interest.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry in \"row\" can be modified to further fine tune results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT to Embed Multi-Sentence Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed individual sentences, we used Sentence-BERT which is a tool extending BERT's base functionality to give \"semantially meaningful sentence embeddings\". The resulting embeddings from this method are comparable using cosine similarity [https://arxiv.org/abs/1908.10084]. \n",
    "\n",
    "In the following section, we demonstrate the use of Sentence-BERT to obtain comparable sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-BERT Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would expect similar sentences to have similar embeddings. For each pair of example sentences, we will measure the cosine similarity of their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "example_set_1 = [\n",
    "    ['I like dogs.', 'I love dogs.'],\n",
    "    ['I like dogs.', 'I want a puppy.'],\n",
    "    ['I like dogs.', 'I like cats.'],\n",
    "    ['I like dogs.', 'I want food.'],\n",
    "    ['I like dogs.', 'The hound ran around the yard.'],\n",
    "    ['I like dogs.', \"It snowed today.\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "from textwrap import fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text(example_set):\n",
    "    \n",
    "    # Initialize table\n",
    "    t = PrettyTable(['Excerpt A', 'Excerpt B', 'Similarity'])\n",
    "    \n",
    "    for e in example_set:\n",
    "        \n",
    "        # Get embeddings for example\n",
    "        embed = model.encode(e)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        sim = 1 - cosine(embed[0], embed[1])\n",
    "        \n",
    "        # Add results to table\n",
    "        t.add_row([fill(e[0], width=35), fill(e[1], width=35), round(sim,3)])\n",
    "        \n",
    "    # Display table\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_text(example_set_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Multi-Sentence Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"LongResponse_Embedding\" notebooks, different methods of embedding responses that may be multi-sentence were considered. For this purpose, we will revist Method 1 which simply passes the full response to the Sentence-BERT encoding function with no special handling.\n",
    "\n",
    "This method is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set_2 = [\n",
    "    [\n",
    "        \"The ground was covered in snow. Icicles hung from branches. Snowflakes kept falling.\",\n",
    "        \"Winter has brought the coldest weather. This blizzard seems endless.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Winter has brought the coldest weather. This blizzard seems endless.\",\n",
    "        \"Rain waters the blooming flowers. Spring is finally here.\"\n",
    "    ],\n",
    "    [\n",
    "        \"Winter has brought the coldest weather. This blizzard seems endless.\",\n",
    "        \"The table was filled with delicious food. I couldn't decide which dish to start with.\"\n",
    "    ],\n",
    "    [\n",
    "        \"The table was filled with delicious food. I couldn't decide which dish to start with.\",\n",
    "        \"The smell coming from the stove is mouth-watering.\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_text(example_set_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we see that the embeddings of short multi-sentence pieces are still useful. For our purposes, the longest of survey responses are still just a few sentences, so we will assume most embeddings will not be too diluted for use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
